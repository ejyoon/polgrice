---
title: "\"I won't lie, it wasn't amazing\": Modeling polite indirect speech"
bibliography: politeness.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 

 \author{{\large \bf Erica J. Yoon}, {\large \bf Michael Henry Tessler}, {\large \bf Noah D. Goodman} \and {\large \bf Michael C. Frank}  \\
         \{ejyoon, mtessler, ngoodman, mcfrank\} @stanford.edu \\ 
         Department of Psychology, Stanford University}

abstract: 
    "Why do people speak politely? Previous work has suggested that people expect others to speak based on their desires to transfer accurate information efficiently (an epistemic goal) and to make the listener feel good (a social goal), sometimes causing them to produce white lies [@yoon2016]. In the current work, we expand on this theory to consider another prominent case of polite speech: indirect remarks using negation (e.g., \"It wasn't amazing\"). With minimal extensions to the previous framework, our formal model suggests that a pragmatic speaker will produce more indirect remarks when the speaker wants to be considerate and informative at the same time. These predictions were borne out in experimental findings for speaker production. This demonstrates that our model generalizes well, and can account for a broader range of polite speech.
    "

keywords:
    "Politeness; computational modeling; communicative goals; pragmatics"
    
output: cogsci2016::cogsci_paper
---

\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
\definecolor{Orange}{RGB}{255,153,0}

\newcommand{\ejy}[1]{\textcolor{Red}{[ejy: #1]}}  
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}  
\newcommand{\mht}[1]{\textcolor{Blue}{[mht: #1]}}  
\newcommand{\mcf}[1]{\textcolor{Orange}{[mcf: #1]}}

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
```

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(binom)
library(rwebppl)
library(jsonlite)
library(readr)
library(coda)
library(magrittr)
library(ggthemes)
library(forcats)
library(langcog)
# set path to working dir
local.path <- "~/Documents/research/polgrice/"
#local.path <- "~/Documents/Research/polgrice_GIT/"
#source(paste(local.path, "experiment/data_analysis/markdown/polgrice_S.R", sep=""))

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}
# HPDhi<- function(s){
hdi_upper <- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}
# HPDlo<- function(s){
hdi_lower <- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
options("scipen"=10)   

```

# Introduction

Language users hear and produce *polite speech* on a daily basis. 
Adults and even young children spontaneously produce requests in polite forms [@clark1980; @axia1985], and speakers use politeness strategies even while arguing, preventing unnecessary offense to their interactants [@holtgraves1997]. 
But being polite conflicts with one important goal of cooperative communication: exchanging information efficiently and accurately [@Grice1975]. 
People tell white lies ("Your new dress is gorgeous!") and produce indirect speech that is longer and more nuanced than the simplest form of their intended message ("I don’t think that dress looks phenomenal on you" as opposed to “That dress looks terrible”) to make others feel good about themselves. 
Speakers risk potential loss of their intended message (indirect speech), intentionally convey wrong information (lies), and suffer inefficiencies all in the service of being polite.
If information transfer was the only currency in communication, a polite speaker would be broke before bedtime. \mht{$\leftarrow$ too much?}
<!-- a cooperative speaker would find polite utterances undesirable because they are potentially misleading.  -->

A *cooperative speaker*, however, can be imagined as one who has an epistemic goal to improve the listener's knowledge state *as well as* a social goal to minimize any potential damage to the hearer's (and the speaker's own) self-image, called *face* [@Brown1987]. 
If the speaker's intended meaning contains no threat to the speaker or listener's face, then the speaker will choose to convey the meaning in an explicit and efficient manner (putting it *on the record*). 
As the degree of face-threat becomes more severe, however, a speaker will choose to be polite by producing more indirect utterances. 

Inspired by this set of ideas, we have argued that language users think about polite speech as reflecting a tradeoff between two goals: information transfer (which we called *epistemic utility*) and face-saving [*social utility*; @yoon2016].
A speaker with a high weight on social utility will try to save her listener’s face: She hides or risks losing information in her intended message by making her utterance false or indirect to some degree.
On the other hand, a speaker with a high weight on epistemic utility prioritizes truthfulness and informativity, and she may risk a loss of the listener's (or the speaker's own) face.
These idaes were formalized in a model of pragmatic language understanding, building on the Rational Speech Act (RSA) theory [for a review, see @goodman2016].
We tested the polite RSA model (pRSA) with the case study of *white lies* (\red{def?}).
The model captured human participants' inferences about a speaker's goals given her utterance (e.g., "the \[*okay*\] talk was "great") and about the world given a speaker's goal and utterance (e.g., the speaker was trying to be nice and she said that talk was "okay").
People attributed the goal to be "nice" increasingly as the speaker's utterance was more positively biased from the true states, and inferred worse true states than the literal meaning when the speaker was described as wanting to be nice.

<!-- We presented a novel computational model that captures the idea that cooperative speakers attempt to balance between the epistemic and social utilities. -->
<!-- To empirically test our model -- which we will refer to as pRSA -- @yoon2016 examined one case study of polite speech: white lies. -->
<!-- We looked at people’s inferences about a speaker who observed someone else’s performance (e.g., their presentation) and either produced a white lie ("It was amazing") or bluntly spoke the truth ("It was terrible") to varying degrees of deviations from the true state (the true rating deserved by the performance). -->

In the current work, we extend our framework to another polite speech act: *indirect speech*. 
White lies are one particular case of polite speech, whereby a speaker tries to save the listener's face through giving false information to some degree.
Instead of explicitly lying, people sometimes try to be more indirect as a way to be polite.
Through indirect speech, a speaker can express an implied meaning that is different from the literal meaning of the utterance [@searle1975].
In this work, we focus on negation ("not"), which has the potential to be indirect.
For instance, "Mark *isn't* the cleanest person I know" may suggest that the speaker thinks Mark is *unclean* rather than literally just not the person Mark knows with the greatest degree of cleanliness.
As such, negation in indirect speech can be used as a hedging or mitigating device to address an undesirable state that is face-threatening to the addressee [@Brown1987; @Grice1975], and can imply that the intended meaning is worse than the vague meaning. 

What may lead a speaker to produce indirect remarks with negation?
If Alice observes Bob’s terrible presentation and decides to say, "it wasn't amazing", this may reflect her attempt to balance between the epistemic and social goals.
Informationally, "not amazing" does not preclude the possibility that the presentation was bad, so the utterance is not a downright lie.
Socially, Alice still tries to save Bob’s face by not being explicitly critical (“It was terrible”).
On the other hand, if the presentation was actually good, or even decent, Alice will prefer to produce a directly positive remark ("It was good”). 
Thus we predict that a speaker who wants to balance between epistemic and social goals would produce indirect speech relatively more, especially when the true state is bad. 
Importantly, we predict it is the interaction between presence of face threat (due to addressee's poor performance) and speaker's consideration of both epistemic and social goals that is predicted to lead to a greater degree of indirect speech production. In what follows, we derive our hypotheses using our formal model and present an empirical test of the hypotheses.

# Computational Model

In the current work, we build on our previous RSA model [pRSA; @yoon2016] with minimal extensions to predict a speaker's production of indirect remarks with negation. 

## Prior work: Description of pRSA

pRSA assumes speaker to choose utterances approximately optimally given a utility function, a standard assumption made in family of RSA models [@Goodman2013]. The model has two utilities considered by the speaker. First, *epistemic utility* ($U_{epi}$) refers to the amount of information a *literal listener* ($L_0$) would still not know about world state $s$ after hearing a speaker's utterance $w$, minus a non-negative cost $C(w)$; Second, *social utility* ($U_{soc}$) is the expected utility of the state the listener would infer given the utterance $w$, which is related to the intrinsic value of the state, where a value function ($V$) maps states to subjective utility values and thus captures the affective consequences for the listener of being in state $s$. Thus we defined the overall speaker utility ($U$) to be a weighted combination of epistemic and social utilities.

$$U(w;s;  \hat{\beta}) = \beta_{epi}\cdot \ln(P_{L_0}(s \mid w)) 
\\+ \beta_{soc} \cdot \mathbb{E}_{P_{L_0}(s \mid w)}[V(s)]- C(w)$$

The speaker ($S_1$) in pRSA chooses utterances $w$ softmax-optimally given the state $s$ and his goal weights $\hat{\beta}$. The pragmatic listener ($L_1$) jointly infers the state $s$ and the utility weights of the speaker, $\beta_{epi}$ and $\beta_{soc}$ [@GoodmanLassiter2015; @Kao2014]. 

$$P_{L_0}(s \mid w)\propto {w}(s) \cdot P(s)$$ 

$$P_{S_1}(w \mid s, \hat{\beta}) \propto \mathrm{exp}(\lambda_{S_1} \cdot \mathbb{E}[U(w; s;  \hat{\beta})])$$

$$P_{L_1}(s,  \hat{\beta} \mid w)\propto P_{S_1}(w \mid s, \hat{\beta})\cdot P(s) \cdot P( \hat{\beta})$$

Within our experimental domain, we assumed there were five possible states of the world corresponding to the value placed on a particular referent (e.g., rating deserved by the presentation the speaker is commenting on): $S = \{s_{1}, ...,  s_{5}\}$. We further assume a uniform prior distribution over possible states of the world. The states have subjective numerical values $V(s_{i}) = \alpha \cdot i$, where $\alpha$ is a free parameter.

## Extensions to pRSA

The current work builds on pRSA by adding three simple but key components to predict indirect remark production. First, we add the pragmatic speaker ($S_2$) who chooses an utterance based on the pragmatic listener model, thinking about the state as well as goal weights that the pragmatic listener will infer.

$$P_{S_2}(w \mid s, \hat{\beta})\propto \mathrm{exp}(\lambda_{S_2} \cdot \ln(P_{L_1}(s,  \hat{\beta} \mid w)) - C(w))$$

Second, there had previously been five possible utterances: {It was *terrible*, *bad*, *okay*, *good*, and *amazing*}, all direct assertions of specific states (e.g., "It was amazing" would be true for the state of 5 but untrue for the states of 1 or 2). To probe people's inferences about indirect remarks, we added five utterances to the set: {It *wasn't* terrible, bad, okay, good, and amazing}. These utterances indirectly address the referent by negating certain state. Third, we assume that it is more costly to say "It wasn't terrible" than "It was amazing" due to inclusion of negation [@GoodmanLassiter2015]. This cost of utterances with negation was accounted for by holding constant the costs of direct remarks with no negation (e.g., cost = 1) and inferring the posterior credible values of the costs of indirect remarks ($w \sim Unif(1,5)$) from data in each experiment. \ejy{FIXME: phi mixture weight.} We implemented this model using the probabilisitic programming language WebPPL [@dippl]\footnote{A complete implementation of the model, links to the experiments, raw data and analyses can be found at \url{https://github.com/ejyoon/cogsci2017}.}.

To confirm our hypotheses, we first generate schematic model predictions on speaker production of indirect speech with negation vs. direct speech with no negation. We compare these model predictions against empirical data in speaker production experiment, where we examine people's expectations for production of utterances $u$ given a true state and a speaker's goal. We confirm that both the model predictions and empirical findings are broadly consistent with our hypothesis that indirect speech reflects a greater balance between epistemic and social goals. 

# Schematic model predictions

We initially test our hypotheses on a schematic model, with fixed goal weights and parameters.

## Semantic measurement

We first probed judgments of literal meanings of the target words assumed by our model and used in all our experiments. We used these judgments to set expected literal meanings of utterances in our initial model predictions, and used them as informative priors to infer literal meaning for the utterances in the experiment for the model fitting.

```{r expt1_results, fig.env = "figure*", fig.pos = "t", fig.width=6, fig.height=2, fig.align = "center", fig.cap = "Semantic measurement results. Proportion of acceptances of utterance types (shown in different colors) combined with target words (shown in different facets) given the true state represented on a scale of hearts. Error bars represent 95\\% confidence intervals."}
img <- png::readPNG("figs/lit_sem.png")
grid::grid.raster(img)

# ms <- read.csv(paste(local.path, "experiment/data_analysis/data/literalSemantics_wNeg.csv", sep="")) %>%
#   mutate(positivity = factor(as.numeric(grepl("yes", utterance)), 
#                              levels = c(0, 1), 
#                              labels = c("it wasn't ~","it was ~"))) %>%
#   mutate(utterance = substring(utterance, 5)) %>%
#   mutate(utterance = ordered(utterance, levels = c("terrible", "bad", "okay", "good", "amazing")))  %>%
#   group_by(positivity, state, utterance, subid) %>%
#   summarize(
#             judgment = mean(judgment, na.rm=TRUE)
#           ) %>%
#   group_by(positivity, state, utterance) %>%
#   multi_boot_standard(column = "judgment") %>%
#   mutate(judgment = mean)
# 
# qplot(state, judgment, 
#       colour = positivity,
#       data=ms) + 
#   geom_line(aes(group=positivity)) +
#   facet_grid(.~utterance) +
#   xlab("state (1=worst)") +
#   ylab("proportion acceptances") +
#   geom_errorbar(aes(ymin=ci_lower,ymax=ci_upper,width=.1)) +
#   scale_color_discrete(guide_legend(title="")) +
#   ggthemes::theme_few()


```

```{r model_pred_negNoneg, fig.env = "figure*", fig.pos = "t", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "Schematic model predictions (left), experimental results (center) and fitted model predictions (right) for average proportion of negation produced among all utterances, given true states (x-axis) and goals (colors)."}
img <- png::readPNG("figs/exptModNeg.png")
grid::grid.raster(img)
```

### Materials and methods 

\ejy{is it okay to use this way of sectioning?} 25 participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk. We used 13 different context items that were previously used in @yoon2016, in which someone evaluated a performance of some kind. For example, in one of the contexts, Bob saw a presentation, and Bob's feelings toward Ann's cake (*true state*) were shown on a scale out of five hearts (e.g., two out of five hearts filled in red color). The question of interest was "Do you think Bob thought the presentation was / wasn't X?" where X could be one of five possible words: *terrible*, *bad*, *okay*, *good*, and *amazing*, giving rise to ten different possible utterances (with negation or no negation). Each participant read 50 scenarios, depicting every possible combination of 5 true states and 10 utterances. Participants indicated their answer to each question by answering "No" or "Yes." The order of context items was randomized, and there were a maximum of four repeats of each context item per participant.

### Results 

For this and next experiments, we analyze the data by collapsing across context items. Meanings of the words as judged by participants were as one would expect (see Figure 1). We used the fraction of participants that endorsed utterance $w$ for state $s$ to set informative priors to infer posterior credible values of the literal meanings from data in the speaker production experiment.

## Model parameters and predictions

To model what speakers would say given true states and their goals (e.g. wanting to make the listener feel good), we assumed a particular set of speaker's goal-weights {$\beta_{epistemic}$, $\beta_{social}$} to represent three goal conditions that we are interested in: *informative* ($\beta_{epistemic}$ = 0.9; $\beta_{social}$ = 0.1); *social* ($\beta_{epistemic}$ = 0.1; $\beta_{social}$ = 0.9); and *both goals* ($\beta_{epistemic}$ = 0.5; $\beta_{social}$ = 0.5). There are four additional parameters of the model, and for the purposes of generating model predictions a priori, we will assign reasonable parameters based on pilot runs of the model-data fit: the speaker optimality parameter ($\lambda_{S_1}$ assigned to 2); the pragmatic speaker optimality parameter ($\lambda_{S_2}$ to 2); the value scale parameter ($\alpha$ to 1) in the utility function; and the cost parameter ($C(u)$ to 2). 

The predictions for the speaker's utterance were consistent with our hypothesis, namely that indirect speech was relatively more preferred given bad true states and speaker's consideration of both epistemic and social goals (Figure 2, left). The model inferred higher likelihood of negation production for the both-goal condition than informative and social conditions when the true state was bad (1 heart). For all goal conditions rate of indirect speech decreased as the true states improved, also as expected; given a good performance, the speaker should produce positive direct remarks (e.g. "It was amazing"). These simplified predictions thus supported our hypothesis that face threat in the true state and both epistemic and social utilities contribute to greater production of indirect speech. Next, we confirm these model predictions in an experiment. 

# Speaker production experiment

To compare against our model predictions, we examined people's predictions for the most likely utterance produced by the speaker ($u$), given a description of the true state of the world (e.g., the speaker felt that a poem deserved 2 out of 5 hearts) and the speaker's goals (e.g., the speaker wanted to make the listener feel good). Critically, the contexts indicated face threats toward the listener, as the speaker's utterance was an evaluation of the listener's performance. We hypothesized that when there is no tradeoff between informativity and face-threat avoidance (i.e. when the addressee's performance is great), speakers should use truthful and face-saving direct remarks ("[Your talk] was amazing") regardless of their described goals. However, when there is a conflict between the epistemic and social goals (i.e. when the addressee's performance is poor), a speaker who tries to compromise between both goals would use vague indirect remarks more often than direct face-threatening remarks (blunt truth; preferred by an informative speaker) or direct face-saving remarks (i.e. white lies; preferred by face-saving speaker). Our hypothesis and method were pre-registered prior to data collection on the Open Science Framework (\url{http://osf.io/b73dm}). 

## Method

### Participants

202 participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk.

### Stimuli and Design

We designed scenarios in which a person (e.g., Ann) gave some performance and asked for another person (e.g., Bob)'s opinion on the performance. The same context items and true states as Experiment 1 were used. Additionally, we provided information on the speaker Bob's goal (*to make Ann feel good*, or *to give as accurate and informative feedback as possible*, or both) and the true state, or how Bob actually felt Ann's performance (e.g., 2 out of 5 hearts), Then we asked participants to predict what Bob would say, out of 10 possible utterances ("It was terrible", "It was bad" ... "It wasn't good", "It wasn't amazing"). Each participant read 15 scenarios, depicting every possible combination of 3 goals and 5 states. The order of context items was randomized, and there were a maximum of two repeats of each context item per participant.

```{r expt2_screen, fig.env = "figure", fig.pos = "H", fig.width=3.5, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Example of a trial in Experiment 1."}
img <- png::readPNG("figs/expt2_screen.png")
grid::grid.raster(img)
```

### Procedure

Participants read each scenario followed by a question that read, "If Bob wanted *to make Ann feel good* (or *to give accurate and informative feedback*, or *BOTH make Sarah feel good AND give accurate and informative feedback*), what would Bob be most likely to say?" Participants indicated their answer by choosing one of the options on the two dropdown menus, side-by-side, one for choosing between *was* vs. *wasn't* and the other for choosing among *terrible*, *bad*, *okay*, *good*, and *amazing* (see Figure 3).

## Behavioral results

```{r expt2_results, fig.env = "figure*", fig.pos = "t", fig.width=7, fig.height=6, fig.align = "center", fig.cap = "Experimental results (\"experiment\" rows) and model predictions (\"model\" rows) for speaker production. Proportion of utterances chosen (utterance type -- direct vs. indirect -- on x-axis and words shown in different colors) given the true state (columns) and speaker goals (rows). Error bars represent 95\\% confidence intervals for the data and 95\\% highest density intervals for the model."}
img <- png::readPNG("figs/exptModUtt.png")
grid::grid.raster(img)
```

Our hypotheses for utterance production by speakers with different goals were borne out: People's predictions for speaker's use of indirect vs. direct remarks given varied depending on speaker's goals, and these differences were especially pronounced for worse true states (Figure 3, center). 
Predictions for individual utterances were also intuitive. For good states (4 and 5 hearts), positive direct remarks were judged to be the most likely utterances across all three goal conditions. For less-than-perfect, but still decent states, there was a greater degree of expectation of white lies (e.g., "It was amazing" for 4 hearts) given social goal (Figure 4, right). 
For bad states (1 and 2 hearts), as we predicted, there were more instances of expected indirect remarks overall across all goal conditions given bad states. Critically, speakers with both goals to be informative and socially considerate produced more indirect than direct remarks, unlike the other two goal conditions. Thus, these results indicated that a speaker who considers both informative and social goals, and thus is in want of a compromise between the two, is expected to produce relatively more indirect remarks.

## Model predictions

### Model fitting

In this experiment, participants were told true states and what speakers' intentions were (e.g., Bob wanted to make Alice feel good). We assume that the intention descriptions conveyed to the participants a particular set of goal-weights {$\beta_{epi}$, $\beta_{soc}$} that the speaker was using. We put uninformative priors on these weights ($\beta$ ~ Uniform(0,1)) and infer their credible values separately for each goal condition ("wanted to X") using Bayesian data analytic techniques [@LW2014].

```{r model_param}
load(paste(local.path, "model/results/cogsci17/bda-s2-mcmc80k-x4-params.RData", sep = ""))

param_summary <- bda.params %>%
  group_by(param) %>%
  summarize(MAP = estimate_mode(val),
            ci_lower = hdi_lower(val),
            ci_upper = hdi_upper(val))
```

There were four additional parameters of the model: the speaker optimality parameter ($\lambda_{S_1}$); the pragmatic speaker optimality parameter ($\lambda_{S_2}$); the value scale parameter ($\alpha$) in the utility function; and the cost parameter ($C(u)$). We put uninformative priors on these ($\lambda_{S_1}$ ~ Uniform(0,20); $\lambda_{S_2}$ ~ Uniform(0,5); $\alpha$ ~ Uniform(0,5); $C(w)$ ~ Uniform(1,10)) and infer their posterior credible values from the data. We ran 4 MCMC chains for 80,000 iterations, discarding the first 40,000 for burnin. The Maximum A-Posteriori (MAP) estimate and 95% Highest Probability Density Interval (HDI) for $\lambda_{S_1}$ are `r round(param_summary$MAP[3], 2)` [`r round(param_summary$ci_lower[3], 2)`, `r round(param_summary$ci_upper[3], 2)`]; for $\lambda_{S_2}$ `r round(param_summary$MAP[4], 2)` [`r round(param_summary$ci_lower[4], 2)`, `r round(param_summary$ci_upper[4], 2)`]; for $\alpha$ `r round(param_summary$MAP[1], 2)` [`r round(param_summary$ci_lower[1], 2)`, `r round(param_summary$ci_upper[1], 2)`]; and for $C(w)$ `r round(param_summary$MAP[2], 2)` [`r round(param_summary$ci_lower[2], 2)`, `r round(param_summary$ci_upper[2], 2)`]. To generate utterance predictions, given our cognitive model and the inferred parameters, we evaluated the posterior predictive distribution, marginalizing out all parameters. 

### Results

```{r model_param2}
load(paste(local.path, "model/results/cogsci17/bda-s2-mcmc80k-x4-goalWeights.RData", sep = ""))

weights_summary <-   bda.goalWeights %>% 
  group_by(param, goal) %>%
  summarize(
    MAP = estimate_mode(val),
            ci_lower = hdi_lower(val),
            ci_upper = hdi_upper(val))
```

The inferred weights for each goal condition were largely as expected: For the "wanted to give informative feedback" (*informative*) condition, the model puts a moderate weight on epistemic utility (`r round(weights_summary$MAP[2], 3)`). For the "wanted to make [the listener] feel good" (*social*) condition, the model infers the speaker was using a moderate weight on epistemic utility (`r round(weights_summary$MAP[3], 3)`). For the "wanted BOTH to make [the listener] feel good and give informative feedback" (*both*) condition, the model assigned a weight on epistemic utility between the weights for the other two goal conditions (`r round(weights_summary$MAP[1], 3)`). Overall, the weights tended to be more biased towards prioritizing the epistemic utility. 

The predictions for the speaker's utterance were broadly consistent with experimental findings (Figure 4). The model successfully predicted distinct patterns for each goal condition. The *informative* speaker produced direct remarks whose literal meanings mapped onto the true states (e.g. "It was terrible" given 1 heart). The *social* speaker produced remarks that were positively biased compared to the true states (e.g. "It was okay" given 2 hearts). The *both-goal* speaker generally produced more indirect remarks with negation than others ("It was terrible" had the highest likelihood given 1 heart). 

However, the model predictions fit to data did not predict the expected difference for negation production between both-goal versus informative and social conditions (Figure 2, right). There are several possible causes for this finding: the social speaker was inferred to place a higher weight on epistemic utility in the experimental data compared to the schematic predictions we used. Thus, the particular goal descriptions we used in the experiment may have suggested the social speaker as being equivalently informative as the both-goal speaker. If this were the case, our argument still holds: a balance between epistemic and social utility, rather than bias toward one utility over the other, is what predicts indirect speech production. Another possible cause is that participants preferred different kind of indirect speech than the model -- the both-goal speaker preferred to produce "It wasn't amazing" in the schematic predictions of the model, whereas participants in our experiment chose "It wasn't terrible" as the most likely utterance produced by the both-goal speaker. This discrepancy between the two remarks is interesting to note, because their implied meaning is similar; in a task of true-state inference (not reported in this paper), participants judged the implied state for "It wasn't amazing" and "It wasn't terrible" to be similar (~2 hearts). What causes this difference in preference for one remark over the other is a remaining question to be explored.

Overall, however, the expected values of the model explain almost all of the variance in the average data $r^2$(15) = 0.962 (Figure 5). Thus, the model predictions supported our hypothesis that face threat in the true state and both epistemic and social utilities contribute to greater production of indirect speech. 

```{r expt2_model, fig.env = "figure", fig.pos = "h", fig.width=3, fig.height=3, fig.align = "center", fig.cap = "Full distribution of human responses vs. model predictions. Error bars represent 95\\% confidence intervals for the data and 95\\% highest density intervals for the model."}
img <- png::readPNG("figs/exptModCor.png")
grid::grid.raster(img)
```

# Discussion

In this work, we showed that our formal model with two speaker utilities (epistemic and social) can be used to not only explain white lie understanding but also indirect remark production and comprehension. Our model on its own predicted that speakers produce more indirect remarks given poorer performance of the addressee (thus involve greater face threat) and speaker's goal to both be informative and to save face, and experimental data confirmed these predictions. Model fit to the data was very strong, although it did not show the predicted dominance of indirect speech for both-goal speaker at low states. Whether this discrepancy between the initial and data-fitted predictions is due to variation in goal weight based on experimental scenarios, or due to discrepancy in particular indirect speech preferred by participants vs. the model, remains to be answered.

An important contribution of this work is in showing the generalizability of our formal model to different kinds of polite speech (white lies and indirect remarks). Can the model be extended to other polite speech phenomena? One significant factor to take into account will be speaker's self-presentation: Not only will a speaker want to save the listener's face, but she also will want to save her own face. Thus, examining how our model's utilities can be extended to speaker's desire to save her own face by appearing polite, genuine, or modest will be an important next step. Using the model to explore other kinds of polite speech such as indirect requests ["Would you mind closing the window?"; @clark1980], agreeable self-presentations (e.g. modesty; "I didn't do too well on my test"), and manifestations of polite speech in different cultures [e.g., @holtgraves1990] will also be important directions for future research.

In sum, our formal model and experimental work present a key contribution to polite speech understanding. With a minimal extension to our existing model, we were able to capture subtle patterns in people's inferences for indirect speech production. Through empirical findings compared against our model predictions on indirect speech, we showed that neither epistemic nor social motives alone motivate indirect speech; instead, the need for indirect speech results from the conflict between these two. These findings provide strong support for our utility-theoretic framing of politeness, and promises next concrete steps in pragmatic understanding.

# Acknowledgments

This work was supported by NSF grant BCS #1456077 to MCF, ONR grant N00014-13-1-0788 and a James S. McDonnell Foundation Scholar Award to NDG, NSF Graduate Research Fellowship DGE-114747 to MHT, and NSERC post-graduate doctoral scholarship PGSD3-454094-2014 to EJY.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
