---
title: "BDA of Polite RSA (RWebPPL)"
author: "M. H. Tessler"
date: "November 3, 2016"
output: html_document
---

```{r setup, include=FALSE}
library(rwebppl)
library(jsonlite)
knitr::opts_chunk$set(echo = TRUE)
library(coda)
estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}
HPDhi<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}
HPDlo<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
options("scipen"=10)   

# set path to working dir
local.path <- "~/Documents/research/polgrice/"
```

# Literal semantics data

```{r}
d.lit <- read.csv(paste(local.path,
                      "experiment/data_analysis/data/literalSemantics_sites.csv",
                      sep = ""))
```

Summarize behavioral data (in terms of number of "yes"es)

```{r}
d.lit.summary <- d.lit %>%
  filter(!is.na(judgment)) %>% # why is there an NA in korea data?
  group_by(site, state, utterance) %>%
  summarize(k = sum(judgment),
            n = n())
```

## Model

```{r}
literalSemanticsModel <- '
var n = data[0]["n"]
var k = data[0]["k"]

var literalSemantics = function(){
	var theta = uniform(0,1)
  observe( Binomial({n: n, p: theta}), k )
  return {theta: theta}
}
'
```

Run model, for all sites, states, utterances

```{r}
sites <- levels(d.lit.summary$site)
states <- levels(factor(d.lit.summary$state))
utterances <- levels(d.lit.summary$utterance)

litSemantics.results <- data.frame()

for (si in sites){
  d.site <- filter(d.lit, site == site)
  for (st in states){
    for (utt in utterances){
      
      d.lit.pass <- d.lit.summary %>% 
        filter((site == si) & (state == st) & (utterance == utt))
      
      rs <- webppl(literalSemanticsModel,
             data = d.lit.pass,
             data_var = "data",
             inference_opts = list(
               method = "rejection",
               samples = 10
             ),
             model_var = "literalSemantics",
             output_format = "samples")
      
      rs.summary <- rs %>%
        summarize(MAP = estimate_mode(theta),
                  cred_low = hdi_lower(theta),
                  cred_upper = hdi_upper(theta)) %>%
        mutate(site = si, state = st, utterance = utt)
      
      litSemantics.results <- bind_rows(litSemantics.results,
                                        rs.summary)
      print(utt)
    }
    print(st)
  }
  print(si)
}
```

Load literal semantics BDA results

```{r}
litSemantics.results <- read.csv(paste(local.path, "model/data/literal_semantics_sites.csv",
                                       sep = ""))
```


# State inference data

```{r}
d.state <- read.csv(paste(
  local.path, 
  "experiment/data_analysis/data/state_sites.csv"
  , sep="")
  )
```

Polite RSA: State inference model

```{r}
pRSA <- '
var states = [1,2,3,4,5];
var weightBins = [0.1,0.3,0.5,0.7,0.9];

var utterances = ["terrible","bad","okay","good","amazing"];

var statePrior = function(){
  return uniformDraw(states);
};

var utterancePrior = function(){
  return uniformDraw(utterances)
  //return utterances[discrete(map(function(u) {return Math.exp(-cost[u]);}, utterances))];
};

// model parameters
// var alpha = 1.25;
// var speakerOptimality = 10;

// measured in Experiment 1
var literalSemantics = dataFromR.literalSemantics;

var meaning = function(words, state){
  return flip(literalSemantics[words][state-1]);
}; 

var listener0 = cache(function(utterance) {
  Infer({method: "enumerate"}, function(){
    var state = statePrior();
    var m = meaning(utterance, state);
    condition(m);
    return state;
  });
}, 10000);

var speaker1 = cache(function(state, speakerGoals, rsaParameters) {
  Infer({method: "enumerate"}, function(){

    // unpack parameters
    var speakerOptimality = rsaParameters.speakerOptimality;
    var alpha = rsaParameters.alpha;

    var utterance = utterancePrior();
    
    var L0 = listener0(utterance);
    
    var epistemicUtility = L0.score(state);
    var socialUtility = expectation(L0, function(s){return alpha*s});
    
    var eUtility = speakerGoals.honesty*epistemicUtility;
    var sUtility = speakerGoals.kindness*socialUtility;
    var mUtility = speakerGoals.meanness*socialUtility;
    
    var speakerUtility = eUtility+sUtility-mUtility;
    
    factor(speakerOptimality*speakerUtility);
    
    return utterance;
  })
}, 10000)

var listener1 = cache(function(exptCondition, rsaParameters) {
  Infer({method: "enumerate"}, function(){
    
    var utterance = exptCondition.utterance;
    var trueState = exptCondition.state;
    var knownGoalsWeights = exptCondition.goalWeights;
    
    var state = statePrior()
    
    // Expt 2. State inference task:
    // goal weights are known (e.g. "speaker is trying to be nice")
    var speakerGoals = knownGoalsWeights ?
    {
      honesty: knownGoalsWeights.honesty,
      kindness: knownGoalsWeights.kindness,
      meanness: knownGoalsWeights.meanness
    } : // expt 3. goal inference task; listener has uncertainty about weights
    {
      honesty: uniformDraw(weightBins),
      kindness: uniformDraw(weightBins),
      meanness: uniformDraw(weightBins)
    }
    
    // Expt 3. goal inference, trueState is known.
    condition(trueState ? trueState == state : true)
    
    var S1 = speaker1(state, speakerGoals, rsaParameters)
    
    observe(S1, utterance)
    
    return state 

  })
}, 10000)
'
```

Open questions about model:
1. What is the right space of weights? (i.e., what is the support of the prior distribution?)
-- If weights are only between 0 - 1
    -- If there is no "mean" parameter, speaker can never be mean
    -- If there is no "dishonest" parameter, speaker can never be lying
-- If weights can go from -1 to 1
    -- For "nice" parameter, speaker can be mean
    -- For "honesty" parameter, speak can be dishonest

For CogSci, MH thinks we had: Weights only between 0 & 1, w/ separate mean parameter

Data analysis model

```{r}
dataAnalysisModel <- '
// foreach helper function
var foreach = function(fn, lst) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};
//

var data = dataFromR.data;

var goals = _.uniq(_.pluck(data, "goal"));
var utterances = _.uniq(_.pluck(data, "utterance"));

var dataAnalysis = function(){

  var RSAparameters = {
    speakerOptimality: uniform(0, 20),
    alpha: uniform(0, 5)
  };

  var goalWeightsAndPostPred = map(function(goal){
    
    var goalWeights = {
      honesty: uniform(0, 1),
      kindness: uniform(0, 1),
      meanness: uniform(0, 1)
    }

    var postPred = map(function(utt){

      var stateData = _.pluck(_.where(data, {utterance: utt, goal: goal}), "state");
  
      var exptConditionInfo = {
        state: false, 
        utterance: utt,
        goalWeights: goalWeights
      };

      var RSApredictions = listener1(exptConditionInfo, RSAparameters);
  
      
      mapData({data: stateData}, function(d){ observe(RSApredictions, d) });
      // observe(dist, val) == factor(dist.score(val)) == condition( sample(dist) == val ) 

      var postPred = expectation(RSApredictions)

      return {key: "posteriorPredictive", goal: goal, utt: utt, val: postPred}

    }, utterances)


    return [postPred, 
      {key: "weightHonest", goal: goal, utt: "NA", val: goalWeights.honesty},
      {key: "weightKind", goal: goal, utt: "NA", val: goalWeights.kindness},
      {key: "weightMean", goal: goal, utt: "NA", val: goalWeights.meanness}
      ]

  }, goals)


  var returnList = _.flatten([goalWeightsAndPostPred, 
        {key: "speakerOptimality", goal: "NA", utt: "NA", val: RSAparameters.speakerOptimality},
        {key: "alpha", goal: "NA", utt: "NA", val: RSAparameters.alpha}
  ])

  var returnObj = _.object(map(function(i){
    [i.key + "_" + i.goal + "_" + i.utt, i.val]
  }, returnList))

  return returnObj

}
'
```

Run Full BDA model

```{r}
sites <- levels(d.state$site)
bda.state.results.allSites <- data.frame()

fullModel <- paste(pRSA, dataAnalysisModel, sep = "\n")

for (si in sites) {
  
  site.data <- filter(d.state, site == si)
  litSemantics <- filter(litSemantics.results, site == si)
  
  litSemantics.toPass <- as.list(litSemantics %>% 
    select(state, utterance, MAP) %>%
    spread(utterance, MAP))
  
  dataToWebPPL <- list(literalSemantics = litSemantics.toPass,
                       data = site.data)
  
  # toJSON(as.list(litSemantics.toPass), pretty = T)
  
  bda.state.results <- webppl(fullModel,
                            data = dataToWebPPL,
                            data_var = "dataFromR",
                            inference_opts = list(method = "MCMC", 
                                                  samples = 2000,
                                                  burn = 1000,
                                                  verbose = TRUE),
                            model_var = "dataAnalysis",
                            output_format = "samples",
                            chains = 2,
                            cores = 2)
  
  bda.state.tidy <- bda.state.results %>% 
    gather(key, val) %>% 
    separate(key, into = c("param", "goal", "utterance"))
    
  
  
  ggplot(bda.state.tidy %>% 
           filter(param == "speakerOptimality"), aes(x = val))+
    geom_histogram()
  
  bda.state.tidy %>% 
    filter(param == "posteriorPredictive") %>%
    mutate(utterance = factor(utterance, 
                              levels = c("terrible", "bad", "okay", "good", "amazing"))) %>%
    group_by(goal, utterance) %>%
    summarize(MAP = estimate_mode(val),
                  cred_low = hdi_lower(val),
                  cred_upper = hdi_upper(val)) %>%
    ggplot(., aes (x = utterance , y = MAP, 
                   ymin = cred_low, ymax = cred_upper, 
                   color = goal, group = goal))+
    geom_point()+
    geom_line()+
    geom_errorbar()
    
  bda.state.results.allSites <- bind_rows(bda.state.results.allSites, 
                                          bda.state.tidy %>% mutate(site = si))
  
}

```



```{r}

```

